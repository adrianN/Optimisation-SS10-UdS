\section{Lagrangean Relaxation}

For an ILP we typically can separate the constraints in two classes, the hard constraints $A$ and the easy constraints $D$\footnote{TODO: switch A and D everywhere. D for difficult :/}. What hard and easy mean depends on the problem, but usually the problem would be solvable in polynomial time if it were not for the hard constraints.

The idea of lagrangean relaxation is to relax the hard constraints, but add a penalty into the objective function for violating them.

For some $\lambda>0$

\begin{align*}
\max \quad & c x + \trans{\lambda}(b-Ax)\\
s.t. \quad & Dx \leq d\\
	& x\in \Z
\end{align*}

If we violated the $Ax\leq b$ constraints we subtract something from our objective value.

This is called the lagrangean relaxation of the problem, the $\lambda_i$ are the Lagrange multipliers.

Since we said that the constraints in $D$ are "easy" we assume that we can solve this problem in polynomial time for some fixed $\lambda$, even without dropping the integrality constraints.

\begin{thm} Let $\lambda \geq 0$ and $z^*,z(\lambda)$ be optimal values of the ILP, LP$_\lambda$ respectively.

\begin{enumerate}
\item the feasible region of the ILP is smaller than the feasible region of LP$_\lambda$
\item $z(\lambda)\geq z^*$, i.e. the relaxed version gives us an upper bound on the optimal value.
\end{enumerate}
\end{thm}

\begin{pr} The first part is rather obvious, since we remove constraints the feasible region can only get larger.

For the second statement let $x^*$ be an optimal solution to the ILP. Since it's optimal it must also be feasible

\[Ax^*\leq b\]

But in the relaxed version we also have some additive term. Then we have

\[z(\lambda) \geq cx^* + \trans{\lambda}\underbrace{(b-Ax^*)}_{\geq 0} \geq cx^*=z^*\]
\qed \end{pr}

So this is a relaxation for every $\lambda$. But since we want to find upper bounds that are as tight as possible, we look for a $\lambda$ that minimises $z(\lambda)-z^*$. Because we know that $z(\lambda)\geq z^*$ for all $\lambda$ we can just minimise $z(\lambda)$. This is called the Lagrangean Dual

\[z_{LD} = \min_{\lambda\geq 0} \{z(\lambda)\} = \min_{\lambda\geq 0} \max_{{x\in \Z}\atop {Dx\leq d}} cx+\trans{\lambda}(b-Ax) \]

For a discussion on how to actually find the optimal $\lambda$ see the book by Tsitsiklis and Bertsimas.

\subsection{Quality of the Lagrangean}

We want to investigate how good the bounds are that we get out of the lagrangean relaxation. First we observe that in some special cases the optimal value of the lagrangian dual can be optimal for the ILP too.

\begin{thm}\label{thm:relaxedOptimal} Let $\lambda \geq 0 $ and let $x_\lambda$ be an optimal solution for the relaxed $LP_\lambda$. If we have

\begin{enumerate}
\item $Ax_\lambda \leq b$
\item $\trans{\lambda}(b-Ax_\lambda) = 0$
\end{enumerate}

then $x_\lambda$ is also an optimal solution for the ILP.
\end{thm}

\begin{pr} Let $x^*$ be an optimal solution to the ILP. By the first condition our penalty term $\trans{\lambda}(b-Ax_\lambda)$ is positive in every component, so for the optimal relaxed value we know: $z_\lambda ^*\geq c x^*$. Since by the second condition we know that the penalty is actually: $\trans{\lambda}(b-Ax_\lambda) = 0$, we know we have equality.

Note: we need the $Ax_\lambda \leq b$ condition because $\lambda$ could be 0.
\qed \end{pr}

Next we compare the quality of the lagrangean relaxation with the easier linear programming relaxation.

\begin{thm} In general

\[z_{LD} \leq z_{LP}\]

Equality holds if the "easy" constraints describe an integral polyhedron.
\end{thm}

\begin{pr} By definition we have

\[z_{LD} = \min_{\lambda \geq 0} z(\lambda) = \min_{\lambda\geq 0} \max_{{x\in \Z}\atop {Dx\leq d}} z(\lambda,x)\]

If the polyhedron $Dx\leq d$ is integral we can just drop the integrality constraint on $x$, otherwise the solution might get better.

\[\min_{\lambda\geq 0} \max_{{x\in \Z}\atop {Dx\leq d}} z(\lambda,x) \geq \min_{\lambda\geq 0} \max_{Dx\leq d} z(\lambda,x)\]

With equality only in the case of integrality.

So we have

\begin{align*}
z_{LD} &\geq \min_{\lambda\geq 0} \max_{Dx\leq d} cx+ \trans{\lambda}(b-Ax)\\
	& = \min_{\lambda \geq 0} (\trans{\lambda}b+\max_{Dx\leq d}(c-\trans{\lambda}A)x)\\
\intertext{Now we dualise the last part. By strong duality the value does not change.}
	& = \min_{\lambda \geq 0} (\trans{\lambda} b+\min_{y\atop {\trans yD=c-\trans{\lambda}A}} \trans y d)\\
	&= \min_{{{\lambda \geq 0}\atop {y\geq 0}}\atop {\trans yD=c-\trans{\lambda}A}} \trans{\lambda} b+ \trans y d\\
	&=\max_{{Ax\leq b}\atop {Dx\leq d}} cx = z_{LP}
\end{align*}
\qed \end{pr}

\subsection{TSP --- Held-Karp-Bound}

We had the following ILP to solve the TSP.

\begin{align*}
\min \quad & \sum_{e\in E} c_{e}x_{e}\\
s.t. \quad & \sum_{e\in \delta(v)} x_{e} = 2 && \forall v\in V\\
	& \sum_{e\in E(S)} x_{e} \leq |S|-1 && S \subseteq \{1,\ldots,n\}, |S|\geq 2, |S| \leq |V|-1\\
	& x_{e} \geq  0
\end{align*}

We can add some redundant constraints without changing the solution

\begin{align*}
\min \quad & \sum_{e\in E} c_{e}x_{e}\\
s.t. \quad & \sum_{e\in E} x_e = n\\
	&\sum_{e\in \delta(v)} x_e=2 && \forall v=\{2,\ldots,n\}\\
	&\sum_{e\in \delta(1)} x_e=2\\
	&\sum_{e\in E(S)} x_e \leq |S|-1 && \forall S \subseteq V, 2\leq |S|\leq |V|-1,1\not i\in S\\
	& x_{e} \geq  0
\end{align*}

The second constraint in this program is the difficult one (as we'll see when we analyse the relaxed problem). We can form the lagrangean relaxation by taking it into the cost function.

For any $\lambda$\footnote{for some reason it's ok to take negative $\lambda$ too\ldots}

\begin{align*}
\min \quad & \sum_{e\in E} c_e x_e + \sum_{v\in \{2,\ldots,n\}} \lambda_v(2-\sum_{e\in \delta(v)} x_e)\\
s.t. \quad & \sum_{e\in E} x_e = n\\
	&\sum_{e\in \delta(1)} x_e=2\\
	&\sum_{e\in E(S)} x_e \leq |S|-1 && \forall S \subseteq V, 2\leq |S|\leq |V|-1,
	& x_{e} \geq  0
\end{align*}

Let's look at the combinatorial structure of any feasible solution for this program.

We claim that any solution for the relaxation is a 1-tree.

\begin{Def} A 1-tree of $G$ is a subgraph $T$ of $G$, s.t.
\begin{enumerate}
\item The degree of node 1 is 2.
\item The subgraph of $T$ on nodes $\{2,\ldots,n\}$ is a tree.
\end{enumerate}
\end{Def}

The second constraint makes sure that node 1 has degree 2. The other constraints make sure that the other part of the solution forms a tree: The last constraint makes sure that there are no cycles and the first and third constraint make sure we have enough edges to form a tree.

We can reformulate the cost function like this, by replacing $\lambda_v$ by $-\lambda_v$

\[z(\lambda,x) = \sum_{e\in E} x_ec_e + \sum_{v\in \{2,\ldots,n\}} \lambda_v (\sum_{e\in \delta(v)} x_e-2)\]

Set $\lambda_1=0$ then

\begin{align*}
z(\lambda,x) &= \sum_{e\in E} x_ec_e + \sum_{v\in V} \lambda_v (\sum_{e\in \delta(v)} x_e-2\sum_{v\in V} \lambda_v)\\
	&=\sum_{e\in E, e=(v,u)} (c_e + \lambda_v + \lambda_u)x_e - 2 \sum_{v\in V}\lambda_v
\end{align*}

Since for a given $\lambda$ the last term is constant, it doesn't matter in our optimisation problem, so we can remove it.

Thus $LP_\lambda$ is the problem of finding a 1-tree of minimal cost with cost function $c_e' = c_{(u,v)} + \lambda_v + \lambda_u$. We can solve this by using some MST algorithm to build the tree on $G=(V\backslash {1},E\backslash \delta(1))$ and adding node 1 back, connecting it with the cheapest edges.

We need to find some good $\lambda$. The algorithm is as follows

\begin{lstlisting}
Let $\lambda=0$, $\omega = 1$
repeat
	H = 1-tree for $c_e' = c_{(u,v)} + \lambda_v + \lambda_u$
	if (H is a tour)
		return $\lambda$
	$\lambda_v = \lambda_v + \omega(\text{deg}(v)-2)$
until $H$ is "good enough"
\end{lstlisting}

if $H$ is a tour we're done by theorem \ref{thm:relaxedOptimal}.