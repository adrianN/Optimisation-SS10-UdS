\subsection*{Tableau Implementation}

\textcolor{red}{Make sure to understand this part, it will be in the midterm.}

Imagine we had the following table at the beginning of an iteration:

\begin{center}
%c is a row vector!
\begin{tabular}{c|c}
$-c_BA_B^{-1}b$ & $c-c_BA_B^{-1}A$\\\hline %don't transpose, it's a rowvector
$A^{-1}_B b$ & $A_B^{-1}A$
\end{tabular}
\end{center}

In the lower right cell we can get the vector $u$. By dividing the components of $u$ by the number in the we column we selected to be introduced into the basis we can find the basic index we need to kick out. It will turn out that computing the new inverse matrices it easy, as they change only in one column from iteration to iteration.

Suppose we have $A_B^{-1}$ and want to find $A_D^{-1}$, where $D=B\cup j \backslash b_i$. Suppose we already had the correct solution with $A_B^{-1}$ 

\[A_B^{-1}A_D = \left[ A_B^{-1}A_{b_i}\ldots A_B^{-1}A_j\ldots A_B^{-1}A_m\right] = \left[ e_1,e_2, \ldots, e_{i-1}, \underbrace{A_B^{-1}A_j}_{=u}, e_{i+1},\ldots, e_m\right]\]

So it's not the correct inverse, but the difference is very small. So we want to find a correction matrix $R$ that gives us the right answer. It is the identity matrix, except for the column that is wrong.

\[R= \begin{pmatrix} 
1 & \ldots & -u_1/u_i & \ldots \\
0 & \ldots & -u_2/u_i & \ldots \\
\vdots & \ddots & \vdots \\
& & 1/u_i & \ldots\\
\vdots & &\vdots &\ddots\\
& & & \ldots & 1
\end{pmatrix}\]

Then we get $RA_B^{-1} = A_D^{-1}$. You can find $R$ yourself by doing gaussian elimination on $A_B^{-1}A_D$ and accumulating the transformation matrices.

\begin{Ex} So now forget for a moment that fancy matrix stuff and see what's going on when you solve an LP by hand.

Suppose we have the following LP

\begin{align*}
\min \qquad& -10 x_1 - 12 x_2 - 12 x_3\\
\text{s.t.}\qquad & x_1 + 2 x_2 +2 x_3 \leq 20\\
& 2x_1 + x_2 + 2 x_3\leq 20\\
& 2x_1 + 2 x_2 + x_3\leq 20\\
& x_1,x_2,x_3 \geq 0
\end{align*}

We convert that to standard form by introducing slack variables:

\begin{align*}
\min \qquad 	  & -10 x_1 -12 x_2 -12 x_3 \\
\text{s.t.}\qquad & x_1 + 2 x_2 + 2 x_3 + x_4 = 20\\
		  & 2x_1 + x_2 + 2 x_3 + x_5  = 20\\
		  & 2 x_1 + 2 x_2 + x_3 + x_6 = 20\\
		  & x_1,x_2,x_3,x_4,x_5,x_6   \geq 0
\end{align*}

Since we can't handle more complicated cases yet, this LP has the origin as a feasible solution. I think you can recognize that by looking at $b$. If everything is positive, the origin is in the feasible region.

So we can start by putting all the slack variables in our basis $B=\{4,5,6\}, A_B=\mathbb{I}$. This is nice because we get the identity matrix as basis and we don't have to invert it to get the initial tableau $A_B^{-1}A$. Our tableau looks like this

\begin{table}[hbt]
\begin{center}
\begin{tabular}{c|cccccc}
  & $x_1$ & $x_2$ & $x_3$ & $x_4$ & $x_5$ & $x_6$ \\\hline
0 & -10 & -12 & -12 & 0 & 0 & 0\\\hline
$x_4=20$ & 1 & 2 & 2 & 1 &  0 & 0 \\
$x_5=20$ & \cellcolor{gruen}{\bf 2} & 1 & 2 & 0 &  1 & 0\\
$x_6=20$ & 2 & 2 & 1 & 0 &  0 & 1\\
\end{tabular}
\end{center}
\caption*{}
\vspace{-1.3cm}
\label{tab:pivotExample}
\end{table}

In the top left cell we see the objective value we currently achieve. Since we negated it, a big value is good here.

The top row shows us the reduced cost vector, i.e. the cost change if we walk along one of the constraints. As expected it is 0 for all basic variables. During the algorithm submatrix for the basic variables should be a diagonal matrix. 

We have to choose one of the non-basic variables with negative reduced cost, for example $x_1$ (there will be a rule to avoid cycling later) and add it to the basis. To decide which element to kick out of the basis we walk over the column of $x_1$ and divide the value of the variable of each row (in the first column) by the value in $x_i$'s column. This is "minimize $|x_{b_i}/d_{b_i}|$ step in the algorithm. It corresponds to rewriting every line of the LP to look for the constraint that becomes tight first. Like this:

\begin{align*}
x_1 + 2 x_2 + 2 x_2 + x_4 = 20 & \quad \Rightarrow & x_4 = 20 - x_1    & \Rightarrow x_1 = 20\\
2 x_1 +x_2 + 2 x_3 + x_5 = 20  & \quad \Rightarrow & x_5 = 20 - 2 x_1  & \Rightarrow x_1 = 10 \\
2 x_1 + 2 x_2 + x_3 + x_6 = 20 & \quad \Rightarrow & x_6 = 20 - 2 x_1  &\Rightarrow x_1 = 10
\end{align*}

Since all the non-basic variables are $0$ and the coefficients of the basic variables always form an identity matrix, each line gives us a simple equation. In this example the basis is degenerate. Since $x_5$ and $x_6$ both achieve the minimum we have to choose one (a rule for this will be introduced later in theorem \ref{Thm:blandsRule}). For example $x_5$. This is called the pivot.

Now we want to introduce $x_1$ into the basis. That means we need to make all coefficients in its column 0 (including the one in the objective function!), except for the one in the line that achieves the minimum. That line gets divided be the coefficient (in an alternative method all other lines get multiplied by the coefficient, that avoids fractions and is called \emph{integer pivoting}). We use the normal row operations we know from gaussian elimination. The '2' in table \ref{tab:pivotExample} at the intersection of the second column ($x_1$) and the fourth row ($x_5$ leaving the basis) is called pivot element.

\begin{center}
\begin{tabular}{c|ccccccr}
  & $x_1$ & $x_2$ & $x_3$ & $x_4$ & $x_5$ & $x_6$ \\\cline{1-7}
0 & -10 & -12 & -12 & 0 & 0 & 0 &\hspace{1cm} $+10\cdot x_1$\\\cline{0-6}
$x_4=20$ & 1 & 2   & 2 & 1 &  0   & 0 &\hspace{1cm} $-1\cdot x_1$\\
$x_1=10$ & 1 & 1/2 & 1 & 0 &  1/2 & 0\\
$x_6=20$ & 2 & 2   & 1 & 0 &  0   & 1 &\hspace{1cm} $-2\cdot x_1$\\
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{c|cccccc}
  & $x_1$ & $x_2$ & $x_3$ & $x_4$ & $x_5$ & $x_6$ \\\hline
100 & 0 & -7 & -2 & 0 & 5 & 0\\\hline
$x_4=10$ & 0 & 3/2 & 1 & 1 & -1/2 & 0 \\
$x_1=10$ & 1 & 1/2 & 1 & 0 &  1/2 & 0\\
$x_6= 0$ & 0 & 1 & -1 & 0 & -1 &  1\\
\end{tabular}
\end{center}

Now we can choose between $x_2$ with $-7$ and $x_3$ with $-2$. $x_2$ wouldn't be good, %why?
so we choose $x_3$ instead. Then we can choose between $x_1$ with $10/1=10$ and $x_4$ with $20/2=10$. Let's use $x_4$. Again we replace $x_4$ on the left by $x_3$, scale that line by $1/2$ (since the coefficient was 2) and do gaussian elimination until we have just a $1$ in that column. That gives us

\begin{center}
\begin{tabular}{c|cccccc}
  & $x_1$ & $x_2$ & $x_3$ & $x_4$ & $x_5$ & $x_6$ \\\hline
120 & 0 & -4 & 0 & 2 & 6 & 0\\\hline
$x_3=10$ & 0 & 3/2 & 1 & 1 & 1/2 & 0 \\
$x_1=0$ &1  & -1 & 0 & -1 &  1 & 0\\
$x_6=10$ & 0 & \cellcolor{gruen}5/2 & 0 & 1 & -3/2 &  1\\
\end{tabular}
\end{center}

The final iteration brings in $x_2$ and kicks out $x_6$

\begin{center}
\begin{tabular}{c|cccccc}
         & $x_1$ & $x_2$ & $x_3$ & $x_4$ & $x_5$ & $x_6$ \\\hline
136      & 0 & 0 & 0 & 3.6 & 1.6 & 1.6 \\\hline
$x_3=4$ & 0 & 0 & 1 & 0.4 & 0.4 & -0.6\\
$x_1=4$  & 1 & 0 & 0 & -0.6 & 0.4 & 0.4\\
$x_2=4$  & 0 & 1 & 0 & 9.4 & -0.6 & 0.4\\
\end{tabular}
\end{center}
\end{Ex}

Now look again at the matrix $R$ from before the example. As you can see it does exactly the same thing as we did by hand to turn the column we selected to enter basis into a unit vector. The updates for the costs and the values of the basic variables get updated by recalculating the equations for them.
