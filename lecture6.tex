\subsection{Cycling}

Sometimes, if the system is degenerate and we're doing unlucky choices, the simplex algorithm can start to cycle.

\begin{Ex}[Cycling]\label{Ex:cycling} In this system we can cycle, if we choose columns in an unlucky way.
\begin{center}
\begin{tabular}{c|cccccc}
    & $x_1$ & $x_2$ & $x_3$ & $x_4$ & $x_5$ & $x_6$\\\hline
    & -2.3 & -2.15 & 13.55 & 0.4 & 0 & 0\\\hline
$x_5$=0 & \cellcolor{gruen}{\bf 0.4} & 0.2 & -1.4 & -0.2 & 1 & 0 \\
$x_6$=0 & -7.8 & -1.4 & 7.8 & 0.4 & 0 & 1\\
\end{tabular}
\end{center}

We select the first column to put it into the basis:

\begin{center}
\begin{tabular}{c|ccccccr}
    & $x_1$ & $x_2$ & $x_3$ & $x_4$ & $x_5$ & $x_6$\\\cline{1-7}
    & 0 & -1 & 5.5 & -0.75 & 5.75 & 0&\hspace{1cm} $\text{old row}_0 + \frac{1.3}{0.4}\cdot \text{old row}_1$\\\cline{1-7}
$x_1$=0 & 1 & 0.5 & -3.5 & -0.5 & 2.5 & 0 &\hspace{1cm} $2.5 \cdot \text{old row}_1$\\
$x_6$=0 & 0 & \cellcolor{gruen}{\bf 2.5} & -19.5 & -3.5 & 19.5 & 1&\hspace{1cm} $\text{old row}_2 + \frac{7.8}{0.4}\cdot \text{old row}_1$\\
\end{tabular}
\end{center}

Now we replace $x_6$ with $x_2$.

\begin{center}
\begin{tabular}{c|ccccccr}
    & $x_1$ & $x_2$ & $x_3$ & $x_4$ & $x_5$ & $x_6$\\\cline{1-7}
    & 0 & 0 & -2.3 & -2.15 & 13.55 & 0.4&\hspace{1cm} $\text{old row}_0 + \frac{1}{2.5}\cdot \text{old row}_2$\\\cline{1-7}
$x_1$=0 & 1 & 0 & 0.4 & 0.2 & -1.4 & -0.2&\hspace{1cm} $\text{old row}_1 + \frac{-0.5}{2.5}\cdot \text{old row}_2$ \\
$x_2$=0 & 0 & 1 & -7.8 & -1.4 & 7.8 & 0.4&\hspace{1cm} $\frac{1}{2.5} \cdot \text{old row}_2$\\
\end{tabular}
\end{center}

This is again the first table, only shifted two columns to the right. Now we could just make the same choices for the pivots and start a cycle (the cycle has length six until we get back to the first table).
\end{Ex}

To avoid the cycling problem exemplified in example \ref{Ex:cycling}, we introduce some additional rules for choosing the pivot element. There are many possibilities for the ruleset. We go with Bland's rule. The reason that we can make those rules is that the Simplex Algorithm has some degree of freedom in choosing the pivot element. 

The choice of the pivoting rule also has implications for the number of iterations until we reach the optimal solution. We'll have a closer look at this in a later section.

\begin{thm}[Bland's Rule]\label{Thm:blandsRule} Suppose simplex picks column 

\[j=\min \{j|\bar c_j < 0\}\]

(i.e. the one with minimal index) and row $i$ such that $u_i>0$, minimises the ratio $x_{b_i}/u_i$ and among those the one with minimal $b_i$. Then simplex never cycles.
\end{thm}

In the second step of example \ref{Ex:cycling} we chose to remove $x_6$ instead of $x_1$ as Bland's rule would mandate.

\begin{pr} Assume the algorithm cycles and w.l.o.g. we start on the cycle. Now remove all variables that are never basic on the cycle, they don't make any difference as they are always 0. Further remove all variables that are always in the basis. The algorithm also has to cycle on this reduced instance. 

It has to be the case that all the remaining variables have value 0. Because the system is degenerate there has to be at least one basic variable that is zero. We always select the variable that minimises $x_{b_i}/u_i$ (for $u_i>0$), so the one with $x_{b_i}=0$ will always be chosen. But since we removed all variables that never leave the basis, all remaining variables have to be 0. 

So according to Bland's rule we always remove the basic variable with the smallest index from the basis, since they all have the same ratio.

We look at two moments in the execution of the algorithm:

\begin{enumerate}
\item \label{blandsRuleMom1} We bring the variable with the largest index $q$ into the basis
\item \label{blandsRuleMom2} We remove $q$ from the basis and add $p$
\end{enumerate}

\paragraph{Observations} In moment \ref{blandsRuleMom1}: $q\not \in B$ and $q$ is the only variable with $<0$ cost (or else we would have chosen the one with lower index. \\
In moment \ref{blandsRuleMom2}: $q\in D$ and $p\not \in D$. Again because of Bland's rule $p$ has to be the entry with lowest index that has a negative cost change. $x_q$, the index we'll kick out, is the first one to have a positive or zero entry in column $p$. In both cases all the basic variables have value 0. %why

Consider 
\[u=A_D^{-1}A_p\]

Remember our observations about the direction $d$ we use to get to the next basis:

\[d_p =1, \quad \forall b_i \in D\ d_{b_i} = -u_i, \quad \forall x\not \in D\cup p\ d_k=0, \quad Ad=0\]

We will compute the  change in cost in two ways. Let $\bar c$ be the old reduced cost vector (moment \ref{blandsRuleMom1}) and $\hat c$ the new one (moment \ref{blandsRuleMom2})

\begin{itemize}
\item First we have 
\[\bar cd = (c - c_B A_B^{-1} A) d= cd - c_B A_B^{-1}\underbrace{Ad}_{=0} = cd = \hat c_p<0\]

\item On the other hand we have
\[\bar c d = \sum_{i=1}^{q} \bar c_i d_i = \underbrace{\bar c_q}_{<0}\underbrace{d_q}_{<0} + \sum _{i=1}^{q-1} \underbrace{\bar c_i}_{\geq 0}\underbrace{d_i}_{\geq 0} \Rightarrow \bar c d >0\]
$d_i$ are $\geq 0$ because our assumption about the entries in column $p$. %TODO

\end{itemize}

So the cost change should be both positive and negative. That is a contradiction.
\end{pr}

So by using Bland's Rule we can remove the assumption that the system is non degenerate and still guarantee termination at an optimal value.

\subsection{Finding an Initial Feasible Basis}

Until now we don't now how to find the initial solution we need to start with the simplex algorithm. We know a method (see section \ref{Sec:FourierMotzkin}) to check whether it is feasible at all, but that wasn't particularly efficient. 

If it is possible to introduce nonnegative slack variables in all constraints the feasible region contains the origin and we can choose the slack variables as initial solution. This is always the case for systems of the form $Ax\leq b$ with $b\geq 0$. In this case the initial basis matrix is the identity matrix and we don't have to work to get the initial tableau.

For an LP with $Ax=b$ and $x\geq 0$ the initial solution is not so obvious. To find it we solve an auxilliary LP that is defined like this.

\begin{align*}
\min \quad & \sum_{i=1}^m y_i\\
&Ax+y=b\\
&x\geq 0
\end{align*}

%TODO for y>=0 the initial basis is just as hard to find as before. We should have a look at the book here.

Note that this LP is not in standard form if some $b_i<0$. In that case $s$ has to be replaced by two vectors, at least in the components that had to be negative. This makes everything more complicated, but it's still straightforward so we'll just assume that $y\geq 0$ is feasible if $x=0$.

Then the initial basis is obvious: Let $x=0$ and $y=b$. Again we get the identity matrix as initial basis matrix and can easily apply the tableau method.

We observe the following things:

\begin{enumerate}
\item If the optimal value in the new LP is $\neq 0$ then the old LP is not feasible because any solution for the original LP gives a trivial assignment for $x$ s.t. all $y_i$ are 0.
\item If the optimal value of the new LP is zero and $(x^*, y^*)$ is an optimal solution, then $x^*$ must be feasible in the old LP. We would like it to be a basis. (Which it may not be, as $y_i$ is not a part of the old LP)
\item If we use the simplex algorithm to solve the new LP we always get an optimal bfs $(x^*,y^*)$. Let $B=\{\kthings{b}{m}\}$ be its basis in the new LP. If every $b_i$ corresponds to some $x$ variable, it is not difficult to see that $B$ is also a feasible basis for the old LP, since the vectors are the same in both LPs. However we may also have some of the $y$ indices. Note that the $y_{b_i}$ have to be zero if the solution is optimal. Hence we have a degenerate bfs as optimal solution.
\end{enumerate}

So it can happen that the bfs we get for the auxilliary LP is not a basis for the original LP because the basis $F$ contains some of the artificial variables $y$. We want to derive a feasible basis from that solution.

Let $A_{b_1},\ldots A_{b_k}$ be the columns of A that are also in the basis $F$. Since $F$ is a basis these columns have to be linearly indepedent. If $A$ has full rank $m$ we can augment the $A_{b_i}$ by $m-k$ additional linearly indepedent vectors from $A$ (as we'll see later we can always transform the LP s.t. the matrix has full rank without changing the feasible region). Since the $y_F$ were zero we can just drop them out of the basis after the augmenting is done and get another basic feasible solution that also gives us a basis for the original problem.

It is not at all obvious how to find the columns of $A$ that we can safely add to our basis. Luckily we can have a hard look at our simplex tableau and spot them. First we describe the procedure, then we check that it's actually correct.

To find a column of $A$ that we can add to our basis first we find a column of $F$ that contains an artificial variable, say $y_l$. Then look at the corresponding row $l$ in the tableau and find a column of $A$, say $A_j$, with a nonzero entry in that row. Using the normal row operations we introduce $x_j$ into the basis and kick out $y_l$. It's ok if the pivot element $(l,j)$ is negative in this step.

So why is this correct? In the tableau we have $F^{-1}\bar A$ (where $\bar A$ is the extended matrix that also contains entries for the artificial variables). So in all the $k$ $A_{b_i}$ that we already have in our basis we have the $i$th unit vector. Since  $k<l$ w.l.o.g. the $l$th entry in these vectors is $0$. That must also be true in any linear combination of the columns $F^{-1}A_{b_1},\ldots, F^{-1}A_{b_k}$. By looking for a column that has a nonzero entry at position $l$ we find a vector $F^{-1}A_j$ that must be linearly independent from all $F^{-1}A_{b_i}$. But since $F^{-1}$ is invertible it must also be independent from $A_{b_i}$ and $A_j$.

It could of course happen that we don't find any entry in the $l$th row that is nonzero. Then the rows of $A$ must linearly depedent since $(F^{-1}A)_l=\trans f_l A=0$. Because the problem is feasible we must also have $\trans f_l b = 0$ and the constraint $\trans f_l A x = \trans f b$ is redundant and can be eliminated by deleting the $l$th row of the tableau. 

It is not so nice that we have to do some post processing to get a basis. A different approach to finding the initial solution is solving the following LP

\begin{align*}
\min \quad & cx + M\sum_{i=1} y_i\\
&Ax+y=b\\
&x,y\geq 0
\end{align*}

where $M$ is a suitably huge constant such that it is bigger than anything except itself. This solution always provides a basis for the original LP, but the implementation is a bit tricky because $M$ has to be handled symbolically.

\subsection{Row Independence}

The remaining problem are cases were the rows of $A$ are not linearly independent. Some of those constraints have to be redundant. We want to find a maximal independent subset $D$ of rows of $A$. Call the remaining rows $F$. We then split $b$ accordingly into $d,f$:

\[A=\left[ D \atop F\right] \qquad b = \left[d\atop f\right]\]

Now take some row $j$ from $F$. We get something that looks like this:

\[a_jx=f_j\]

$a_j$ is a linear combination of rows from $D$

\[a_j=\trans z D\]

where $z$ is a vector of coefficients. If $\trans z d = f_i$ there is $Dx=d$ otherwise $\trans z d \neq f_i$ there is no $x$ s.t. $Dx=d$ and the LP is infeasible.